{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vnghia/5ma-hddl/blob/main/notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib pandas numpy Pillow torch pytorch-lightning albumentations ensemble-boxes tqdm object-detection-metrics rich shapely\n",
    "!pip install -q git+https://github.com/rwightman/efficientdet-pytorch.git\n",
    "!pip install -q gdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/vnghia/PeopleArt.git\" \"people-art\" 2>/dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from effdet import create_model\n",
    "from ensemble_boxes import ensemble_boxes_wbf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from podm.metrics import get_pascal_voc_metrics, BoundingBox\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "import gdown\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper class for reading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtDatasetAdapter:\n",
    "    def __init__(self, data_dir, keep_train_empty_rate=0, keep_test_empty_rate=1):\n",
    "        self.keep_train_empty_rate = keep_train_empty_rate\n",
    "        self.keep_test_empty_rate = keep_test_empty_rate\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_dir = self.data_dir / \"JPEGImages\"\n",
    "        self.annotation_dir = self.data_dir / \"Annotations\"\n",
    "        self.__read_image_list__()\n",
    "        self.__read_annotation__()\n",
    "\n",
    "    def __read_image_list__(self):\n",
    "        self.image_list = defaultdict(list)\n",
    "        for image_type in [\"test\", \"train\", \"trainval\", \"val\", \"own_test\"]:\n",
    "            with open(self.annotation_dir / f\"person_{image_type}.txt\", \"r\") as f:\n",
    "                merge_image_type = image_type\n",
    "                if image_type == \"trainval\":\n",
    "                    merge_image_type = \"train\"\n",
    "                if image_type == \"own_test\":\n",
    "                    merge_image_type = \"test\"\n",
    "                self.image_list[merge_image_type] += [\n",
    "                    line.split(\" \", maxsplit=1)[0]\n",
    "                    for line in f.readlines()\n",
    "                    if (merge_image_type == \"val\")\n",
    "                    or (image_type == \"own_test\")\n",
    "                    or (\n",
    "                        merge_image_type == \"train\"\n",
    "                        and (\n",
    "                            int(line.split(\" \", maxsplit=1)[1]) == 1\n",
    "                            or self.keep_train_empty_rate >= random.random()\n",
    "                        )\n",
    "                    )\n",
    "                    or (\n",
    "                        merge_image_type == \"test\"\n",
    "                        and (\n",
    "                            int(line.split(\" \", maxsplit=1)[1]) == 1\n",
    "                            or self.keep_test_empty_rate >= random.random()\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "    def __read_annotation__(self):\n",
    "        annotation_items = []\n",
    "\n",
    "        for dir in self.annotation_dir.iterdir():\n",
    "            if not dir.is_dir():\n",
    "                continue\n",
    "            for file in dir.iterdir():\n",
    "                annotation = ET.parse(file)\n",
    "                filename = os.path.join(\n",
    "                    annotation.find(\"folder\").text, annotation.find(\"filename\").text\n",
    "                )\n",
    "                objects = annotation.findall(\"object\")\n",
    "                for object in objects:\n",
    "                    bndbox = object.find(\"bndbox\")\n",
    "                    annotation_items.append(\n",
    "                        (\n",
    "                            filename,\n",
    "                            int(bndbox[0].text),\n",
    "                            int(bndbox[1].text),\n",
    "                            int(bndbox[2].text),\n",
    "                            int(bndbox[3].text),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        self.annotation_df = pd.DataFrame(\n",
    "            annotation_items,\n",
    "            columns=[\"image\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"],\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_rectangle_edges_from_pascal_bbox(cls, bbox):\n",
    "        xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
    "\n",
    "        bottom_left = (xmin_top_left, ymax_bottom_right)\n",
    "        width = xmax_bottom_right - xmin_top_left\n",
    "        height = ymin_top_left - ymax_bottom_right\n",
    "\n",
    "        return bottom_left, width, height\n",
    "\n",
    "    @classmethod\n",
    "    def draw_pascal_voc_bboxes(cls, plot_ax, class_labels, bboxes):\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            if class_labels[i] < 0:\n",
    "                continue\n",
    "            bottom_left, width, height = cls.get_rectangle_edges_from_pascal_bbox(bbox)\n",
    "\n",
    "            rect_1 = patches.Rectangle(\n",
    "                bottom_left,\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=4,\n",
    "                edgecolor=\"black\",\n",
    "                fill=False,\n",
    "            )\n",
    "            rect_2 = patches.Rectangle(\n",
    "                bottom_left,\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=2,\n",
    "                edgecolor=\"white\",\n",
    "                fill=False,\n",
    "            )\n",
    "\n",
    "            # Add the patch to the Axes\n",
    "            plot_ax.add_patch(rect_1)\n",
    "            plot_ax.add_patch(rect_2)\n",
    "\n",
    "    @classmethod\n",
    "    def show_image_bboxes(cls, image, class_labels=None, bboxes=None, figsize=(10, 10)):\n",
    "        fig, ax = plt.subplots(1, figsize=figsize)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        if bboxes is not None:\n",
    "            cls.draw_pascal_voc_bboxes(ax, class_labels, bboxes)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def get_image_and_labels_by_idx(self, index, image_type):\n",
    "        index = (\n",
    "            self.image_list[image_type].index(index) if type(index) == str else index\n",
    "        )\n",
    "        image_name = self.image_list[image_type][index]\n",
    "        image = Image.open(self.image_dir / image_name)\n",
    "        pascal_bboxes = self.annotation_df[self.annotation_df.image == image_name][\n",
    "            [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "        ].values\n",
    "        class_labels = np.ones(len(pascal_bboxes))\n",
    "        image_w, image_h = image.size\n",
    "\n",
    "        if len(pascal_bboxes) == 0:\n",
    "            pascal_bboxes = np.array([[0, 0, 1, 1]])\n",
    "            class_labels = np.zeros((1,))\n",
    "\n",
    "        return image, pascal_bboxes, class_labels, index, image_w, image_h\n",
    "\n",
    "    def show_image(self, index, image_type=\"train\"):\n",
    "        image, bboxes, class_labels, image_id, _, _ = self.get_image_and_labels_by_idx(\n",
    "            index, image_type\n",
    "        )\n",
    "        self.show_image_bboxes(image, class_labels, bboxes.tolist())\n",
    "\n",
    "    def len(self, image_type):\n",
    "        return len(self.image_list[image_type])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_ds_adapter = ArtDatasetAdapter(\"people-art\", 0.05, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning helper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDetDataset(Dataset):\n",
    "    def __init__(self, dataset_adaptor, transforms, image_type):\n",
    "        self.ds = dataset_adaptor\n",
    "        self.transforms = transforms\n",
    "        self.image_type = image_type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (\n",
    "            image,\n",
    "            pascal_bboxes,\n",
    "            class_labels,\n",
    "            image_id,\n",
    "            image_w,\n",
    "            image_h,\n",
    "        ) = self.ds.get_image_and_labels_by_idx(index, self.image_type)\n",
    "\n",
    "        sample = {\n",
    "            \"image\": np.array(image, dtype=np.float32),\n",
    "            \"bboxes\": pascal_bboxes,\n",
    "            \"labels\": class_labels,\n",
    "        }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        sample[\"bboxes\"] = np.array(sample[\"bboxes\"])\n",
    "        image = sample[\"image\"]\n",
    "        pascal_bboxes = sample[\"bboxes\"]\n",
    "        labels = sample[\"labels\"]\n",
    "\n",
    "        _, new_h, new_w = image.shape\n",
    "        sample[\"bboxes\"][:, [0, 1, 2, 3]] = pascal_bboxes[\n",
    "            :, [1, 0, 3, 2]\n",
    "        ]  # convert to yxyx\n",
    "\n",
    "        target = {\n",
    "            \"bboxes\": torch.as_tensor(sample[\"bboxes\"], dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels),\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"image_size\": (new_h, new_w),\n",
    "            \"image_scale\": torch.tensor([1.0]),\n",
    "            \"image_original_size\": (image_h, image_w),\n",
    "        }\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.len(self.image_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDetDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataset_adaptor,\n",
    "        validation_dataset_adaptor,\n",
    "        test_dataset_adaptor,\n",
    "        image_size,\n",
    "        train_transforms=None,\n",
    "        valid_transforms=None,\n",
    "        test_transforms=None,\n",
    "        num_workers=4,\n",
    "        batch_size=8,\n",
    "    ):\n",
    "        self.image_size = image_size\n",
    "        self.train_ds = train_dataset_adaptor\n",
    "        self.valid_ds = validation_dataset_adaptor\n",
    "        self.test_ds = test_dataset_adaptor\n",
    "        self.train_transforms = train_transforms or self.get_train_transforms()\n",
    "        self.valid_transforms = valid_transforms or self.get_valid_transforms()\n",
    "        self.test_transforms = test_transforms or self.valid_transforms\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_ds = EfficientDetDataset(\n",
    "            dataset_adaptor=self.train_ds,\n",
    "            transforms=self.train_transforms,\n",
    "            image_type=\"train\",\n",
    "        )\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        self.val_ds = EfficientDetDataset(\n",
    "            dataset_adaptor=self.valid_ds,\n",
    "            transforms=self.valid_transforms,\n",
    "            image_type=\"val\",\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        self.test_ds = EfficientDetDataset(\n",
    "            dataset_adaptor=self.test_ds,\n",
    "            transforms=self.test_transforms,\n",
    "            image_type=\"test\",\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def get_train_transforms(self):\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.Transpose(p=0.5),\n",
    "                A.Resize(height=self.image_size, width=self.image_size, p=1),\n",
    "                A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(p=1),\n",
    "            ],\n",
    "            p=1.0,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\",\n",
    "                label_fields=[\"labels\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_valid_transforms(self):\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(height=self.image_size, width=self.image_size, p=1),\n",
    "                A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(p=1),\n",
    "            ],\n",
    "            p=1.0,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format=\"pascal_voc\",\n",
    "                label_fields=[\"labels\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train_dataset(self) -> EfficientDetDataset:\n",
    "        return self.train_ds\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataset(self) -> EfficientDetDataset:\n",
    "        return self.val_ds\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataset(self) -> EfficientDetDataset:\n",
    "        return self.test_ds\n",
    "\n",
    "    def test_dataloader(self, batch_size=None) -> DataLoader:\n",
    "        return self.test_loader\n",
    "\n",
    "    def get_item(self, image_type, idx):\n",
    "        dataset = getattr(self, f\"{image_type}_ds\")\n",
    "        (\n",
    "            original_image,\n",
    "            pascal_bboxes,\n",
    "            class_labels,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = dataset.ds.get_image_and_labels_by_idx(idx, dataset.image_type)\n",
    "        image, target, image_id = dataset[idx]\n",
    "        image_tensor, annotations, _, _ = self.collate_fn([[image, target, image_id]])\n",
    "        return (\n",
    "            original_image,\n",
    "            pascal_bboxes,\n",
    "            class_labels,\n",
    "            image_tensor,\n",
    "            annotations[\"image_original_size\"],\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, targets, image_ids = tuple(zip(*batch))\n",
    "        images = torch.stack(images)\n",
    "        images = images.float()\n",
    "\n",
    "        boxes = [target[\"bboxes\"].float() for target in targets]\n",
    "        labels = [target[\"labels\"].float() for target in targets]\n",
    "        image_size = torch.tensor([target[\"image_size\"] for target in targets]).float()\n",
    "        image_scale = torch.tensor(\n",
    "            [target[\"image_scale\"] for target in targets]\n",
    "        ).float()\n",
    "        image_original_size = torch.tensor(\n",
    "            [target[\"image_original_size\"] for target in targets]\n",
    "        ).float()\n",
    "\n",
    "        annotations = {\n",
    "            \"bbox\": boxes,\n",
    "            \"cls\": labels,\n",
    "            \"img_size\": image_size,\n",
    "            \"img_scale\": image_scale,\n",
    "            \"image_original_size\": image_original_size,\n",
    "        }\n",
    "\n",
    "        return images, annotations, targets, image_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDetModelWrapper(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dm,\n",
    "        num_classes=1,\n",
    "        prediction_confidence_threshold=0.2,\n",
    "        learning_rate=0.0002,\n",
    "        wbf_iou_threshold=0.44,\n",
    "        model_name=\"efficientdetv2_ds\",\n",
    "        title=None,\n",
    "        model=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dm = dm\n",
    "        self.image_size = self.dm.image_size\n",
    "        self.model_name = model_name\n",
    "        self.title = title or self.model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model or create_model(\n",
    "            self.model_name,\n",
    "            \"train\",\n",
    "            self.num_classes,\n",
    "            bench_labeler=True,\n",
    "            pretrained_backbone=True,\n",
    "        )\n",
    "        self.prediction_confidence_threshold = prediction_confidence_threshold\n",
    "        self.lr = learning_rate\n",
    "        self.wbf_iou_threshold = wbf_iou_threshold\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, annotations, _, image_ids = batch\n",
    "\n",
    "        losses = self.model(images, annotations)\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": losses[\"class_loss\"].detach(),\n",
    "            \"box_loss\": losses[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            losses[\"loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_class_loss\",\n",
    "            losses[\"class_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_box_loss\",\n",
    "            losses[\"box_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return losses[\"loss\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx, log=True):\n",
    "        images, annotations, targets, image_ids = batch\n",
    "        outputs = self.model(images, annotations)\n",
    "\n",
    "        detections = outputs[\"detections\"]\n",
    "\n",
    "        batch_predictions = {\n",
    "            \"predictions\": detections,\n",
    "            \"targets\": targets,\n",
    "            \"image_ids\": image_ids,\n",
    "        }\n",
    "\n",
    "        if log:\n",
    "            logging_losses = {\n",
    "                \"class_loss\": outputs[\"class_loss\"].detach(),\n",
    "                \"box_loss\": outputs[\"box_loss\"].detach(),\n",
    "            }\n",
    "\n",
    "            self.log(\n",
    "                \"valid_loss\",\n",
    "                outputs[\"loss\"],\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"valid_class_loss\",\n",
    "                logging_losses[\"class_loss\"],\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "            self.log(\n",
    "                \"valid_box_loss\",\n",
    "                logging_losses[\"box_loss\"],\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "        return {\"loss\": outputs[\"loss\"], \"batch_predictions\": batch_predictions}\n",
    "\n",
    "    def predict(self, images_tensor, image_sizes=None):\n",
    "        if images_tensor.ndim == 3:\n",
    "            images_tensor = images_tensor.unsqueeze(0)\n",
    "        if (\n",
    "            images_tensor.shape[-1] != self.image_size\n",
    "            or images_tensor.shape[-2] != self.image_size\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Input tensors must be of shape (N, 3, {self.image_size}, {self.image_size})\"\n",
    "            )\n",
    "\n",
    "        return self.__run_inference(images_tensor, image_sizes)\n",
    "\n",
    "    def __run_inference(self, images_tensor, image_sizes=None):\n",
    "        num_images = images_tensor.shape[0]\n",
    "\n",
    "        dummy_targets = self._create_dummy_inference_targets(num_images=num_images)\n",
    "\n",
    "        detections = self.model(images_tensor, dummy_targets)[\"detections\"]\n",
    "        (\n",
    "            predicted_bboxes,\n",
    "            predicted_class_confidences,\n",
    "            predicted_class_labels,\n",
    "        ) = self.post_process_detections(detections)\n",
    "\n",
    "        scaled_bboxes = self.rescale_bboxes(\n",
    "            predicted_bboxes=predicted_bboxes, image_sizes=image_sizes\n",
    "        )\n",
    "\n",
    "        return scaled_bboxes, predicted_class_labels, predicted_class_confidences\n",
    "\n",
    "    def _create_dummy_inference_targets(self, num_images):\n",
    "        dummy_targets = {\n",
    "            \"bbox\": [\n",
    "                torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=self.device)\n",
    "                for i in range(num_images)\n",
    "            ],\n",
    "            \"cls\": [torch.tensor([1.0], device=self.device) for i in range(num_images)],\n",
    "            \"img_size\": torch.tensor(\n",
    "                [(self.image_size, self.image_size)] * num_images, device=self.device\n",
    "            ).float(),\n",
    "            \"img_scale\": torch.ones(num_images, device=self.device).float(),\n",
    "        }\n",
    "\n",
    "        return dummy_targets\n",
    "\n",
    "    def post_process_detections(self, detections):\n",
    "        predictions = []\n",
    "        for i in range(detections.shape[0]):\n",
    "            predictions.append(\n",
    "                self._postprocess_single_prediction_detections(detections[i])\n",
    "            )\n",
    "\n",
    "        (\n",
    "            predicted_bboxes,\n",
    "            predicted_class_confidences,\n",
    "            predicted_class_labels,\n",
    "        ) = self.__run_wbf(predictions, iou_thr=self.wbf_iou_threshold)\n",
    "\n",
    "        return predicted_bboxes, predicted_class_confidences, predicted_class_labels\n",
    "\n",
    "    def _postprocess_single_prediction_detections(self, detections):\n",
    "        boxes = detections.detach().cpu().numpy()[:, :4]\n",
    "        scores = detections.detach().cpu().numpy()[:, 4]\n",
    "        classes = detections.detach().cpu().numpy()[:, 5]\n",
    "        indexes = np.where(\n",
    "            (scores > self.prediction_confidence_threshold) & (classes == 1)\n",
    "        )[0]\n",
    "\n",
    "        return {\n",
    "            \"boxes\": boxes[indexes],\n",
    "            \"scores\": scores[indexes],\n",
    "            \"classes\": classes[indexes],\n",
    "        }\n",
    "\n",
    "    def rescale_bboxes(self, predicted_bboxes, image_sizes=None):\n",
    "        if image_sizes is None:\n",
    "            return predicted_bboxes\n",
    "        scaled_bboxes = []\n",
    "        for bboxes, image_dims in zip(predicted_bboxes, image_sizes):\n",
    "            im_h, im_w = image_dims\n",
    "\n",
    "            if len(bboxes) > 0:\n",
    "                scaled_bboxes.append(\n",
    "                    (\n",
    "                        np.array(bboxes)\n",
    "                        * [\n",
    "                            im_w / self.image_size,\n",
    "                            im_h / self.image_size,\n",
    "                            im_w / self.image_size,\n",
    "                            im_h / self.image_size,\n",
    "                        ]\n",
    "                    ).tolist()\n",
    "                )\n",
    "            else:\n",
    "                scaled_bboxes.append(bboxes)\n",
    "\n",
    "        return scaled_bboxes\n",
    "\n",
    "    def __run_wbf(self, predictions, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
    "        bboxes = []\n",
    "        confidences = []\n",
    "        class_labels = []\n",
    "\n",
    "        for prediction in predictions:\n",
    "            boxes = [(prediction[\"boxes\"] / self.image_size).tolist()]\n",
    "            scores = [prediction[\"scores\"].tolist()]\n",
    "            labels = [prediction[\"classes\"].tolist()]\n",
    "\n",
    "            boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(\n",
    "                boxes,\n",
    "                scores,\n",
    "                labels,\n",
    "                weights=weights,\n",
    "                iou_thr=iou_thr,\n",
    "                skip_box_thr=skip_box_thr,\n",
    "            )\n",
    "            boxes = boxes * (self.image_size - 1)\n",
    "            bboxes.append(boxes.tolist())\n",
    "            confidences.append(scores.tolist())\n",
    "            class_labels.append(labels.tolist())\n",
    "\n",
    "        return bboxes, confidences, class_labels\n",
    "\n",
    "    def eval_model(self, loader):\n",
    "        predicted_bboxes = []\n",
    "        ground_truth_bboxes = []\n",
    "        for batch in tqdm(loader):\n",
    "            image_tensor, annotations, targets, image_ids = batch\n",
    "            (\n",
    "                scaled_bboxes,\n",
    "                predicted_class_labels,\n",
    "                predicted_class_confidences,\n",
    "            ) = self.predict(image_tensor)\n",
    "            for i, target in enumerate(targets):\n",
    "                bboxes, labels, image_id, _, _, _ = target.values()\n",
    "                for bbox, label in zip(bboxes[..., [1, 0, 3, 2]], labels):\n",
    "                    ground_truth_bboxes.append(\n",
    "                        BoundingBox.of_bbox(\n",
    "                            int(image_id[0]),\n",
    "                            int(label),\n",
    "                            bbox[0],\n",
    "                            bbox[1],\n",
    "                            bbox[2],\n",
    "                            bbox[3],\n",
    "                        )\n",
    "                    )\n",
    "                bboxes, labels, confidences = (\n",
    "                    scaled_bboxes[i],\n",
    "                    predicted_class_labels[i],\n",
    "                    predicted_class_confidences[i],\n",
    "                )\n",
    "                for bbox, label, confidence in zip(bboxes, labels, confidences):\n",
    "                    predicted_bboxes.append(\n",
    "                        BoundingBox.of_bbox(\n",
    "                            int(image_id[0]),\n",
    "                            int(label),\n",
    "                            bbox[0],\n",
    "                            bbox[1],\n",
    "                            bbox[2],\n",
    "                            bbox[3],\n",
    "                            confidence,\n",
    "                        )\n",
    "                    )\n",
    "        aps = []\n",
    "        for i in range(10):\n",
    "            iou_threshold = 0.5 + i * 0.05\n",
    "            aps.append(\n",
    "                get_pascal_voc_metrics(\n",
    "                    ground_truth_bboxes, predicted_bboxes, iou_threshold\n",
    "                )[1].ap\n",
    "            )\n",
    "        return {\"AP.5\": aps[0], \"AP.75\": aps[5], \"AP.5:.95:.05\": sum(aps) / len(aps)}\n",
    "\n",
    "    @classmethod\n",
    "    def get_pretrained_wrapper(cls, model_name, image_size, art_ds_adapter, **kwargs):\n",
    "        pretrained_model = create_model(\n",
    "            model_name,\n",
    "            \"train\",\n",
    "            pretrained=True,\n",
    "            bench_labeler=True,\n",
    "        )\n",
    "        pretrained_model_wrapper = cls(\n",
    "            dm=EfficientDetDataModule(\n",
    "                train_dataset_adaptor=art_ds_adapter,\n",
    "                validation_dataset_adaptor=art_ds_adapter,\n",
    "                test_dataset_adaptor=art_ds_adapter,\n",
    "                image_size=image_size,\n",
    "                num_workers=1,\n",
    "                batch_size=4,\n",
    "            ),\n",
    "            num_classes=1,\n",
    "            model=pretrained_model,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return pretrained_model_wrapper.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_bboxes_for_image(\n",
    "        image_type, image_index, figsize=(20, 20), *args, ncols=2\n",
    "    ):\n",
    "        wrapper_count = len(args)\n",
    "        nrows = int((wrapper_count + 1) / ncols + 0.5)\n",
    "        _, axes = plt.subplots(\n",
    "            nrows, ncols, figsize=figsize, squeeze=False, sharex=True, sharey=True\n",
    "        )\n",
    "        for i, wrapper in enumerate(args):\n",
    "            (\n",
    "                original_image,\n",
    "                pascal_bboxes,\n",
    "                class_labels,\n",
    "                image_tensor,\n",
    "                image_sizes,\n",
    "            ) = wrapper.dm.get_item(image_type, image_index)\n",
    "            if i == 0:\n",
    "                gt_ax = axes[0, 0]\n",
    "                gt_ax.imshow(original_image)\n",
    "                gt_ax.set_title(\"Ground Truth\")\n",
    "                gt_ax.set_xticks([])\n",
    "                gt_ax.set_yticks([])\n",
    "                art_ds_adapter.draw_pascal_voc_bboxes(\n",
    "                    gt_ax, class_labels, pascal_bboxes\n",
    "                )\n",
    "            (\n",
    "                predicted_bboxes,\n",
    "                predicted_class_labels,\n",
    "                predicted_class_confidences,\n",
    "            ) = wrapper.predict(image_tensor, image_sizes)\n",
    "            ax = axes[int((i + 1) / ncols), (i + 1) % ncols]\n",
    "            ax.imshow(original_image)\n",
    "            ax.set_title(wrapper.title)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            art_ds_adapter.draw_pascal_voc_bboxes(\n",
    "                ax, predicted_class_labels[0], predicted_bboxes[0]\n",
    "            )\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = EfficientDetModelWrapper(\n",
    "    dm=EfficientDetDataModule(\n",
    "        train_dataset_adaptor=art_ds_adapter,\n",
    "        validation_dataset_adaptor=art_ds_adapter,\n",
    "        test_dataset_adaptor=art_ds_adapter,\n",
    "        image_size=768,\n",
    "        num_workers=1,\n",
    "        batch_size=4,\n",
    "    ),\n",
    "    num_classes=1,\n",
    "    model_name=\"efficientdetv2_dt\",\n",
    "    title=\"Our EfficientDetv2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(f\"epoch=199-step=26800-model={model_wrapper.model_name}.ckpt\")\n",
    "\n",
    "# If gdown can not download this link, please download at\n",
    "# https://drive.google.com/file/d/16OQ8J6o66Y2Ac3--imYZXq_OpuLeVuvX/view\n",
    "if not ckpt_path.exists() and bool(os.environ.get(\"USE_PRETRAINED_MODEL\", \"1\")):\n",
    "    gdown.download(\n",
    "        \"https://drive.google.com/uc?export=download&confirm=pbef&id=16OQ8J6o66Y2Ac3--imYZXq_OpuLeVuvX\",\n",
    "        output=str(ckpt_path),\n",
    "        quiet=False,\n",
    "    )\n",
    "\n",
    "if not ckpt_path.exists():\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        max_epochs=200,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    trainer.fit(model_wrapper, datamodule=model_wrapper.dm)\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "else:\n",
    "    model_wrapper = EfficientDetModelWrapper.load_from_checkpoint(\n",
    "        ckpt_path,\n",
    "        dm=model_wrapper.dm,\n",
    "        num_classes=model_wrapper.num_classes,\n",
    "        model_name=model_wrapper.model_name,\n",
    "        title=model_wrapper.title,\n",
    "    )\n",
    "_ = model_wrapper.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pretrained EfficientDet backbone EfficientNetv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "effdet_v2_dt_wrapper = EfficientDetModelWrapper.get_pretrained_wrapper(\n",
    "    model_wrapper.model_name,\n",
    "    model_wrapper.image_size,\n",
    "    art_ds_adapter,\n",
    "    title=\"EfficientDetv2\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pretrained EfficientDet backbone EfficientNetv1 D4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "effdet_v1_d4_wrapper = EfficientDetModelWrapper.get_pretrained_wrapper(\n",
    "    \"tf_efficientdet_d4\", 1024, art_ds_adapter, title=\"EfficientDetv1 D4\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show an example output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = \"DongHo/danh-ghen.jpg\"\n",
    "EfficientDetModelWrapper.compare_bboxes_for_image(\n",
    "    \"test\",\n",
    "    example_index,\n",
    "    (20, 20),\n",
    "    model_wrapper,\n",
    "    effdet_v2_dt_wrapper,\n",
    "    effdet_v1_d4_wrapper,\n",
    "    ncols=2,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc8cc2aa0891c63261533941cf1db23b317a72ed1160fcec97317053361bacfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
